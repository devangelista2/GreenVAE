# -*- coding: utf-8 -*-
"""Naive VAE (fixed gamma) CIFAR10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lxgdp6qSCP3D5LJfZR9885VgxmHhqy_b
"""

# LIBRARIES
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Dense, Flatten, Reshape, Conv2DTranspose, Lambda
from tensorflow.keras.datasets import cifar10 
import tensorflow.keras.backend as K
from tensorflow.keras.callbacks import Callback

from tensorflow.python.framework.ops import disable_eager_execution
disable_eager_execution()

from profiler import *
profiler = Profiler()
profiler.compute_flops(vae)

# Parameters
input_dim = (32, 32, 3)
latent_dim = 128

epochs = 150
batch_size = 100

initial_lr = 1e-4
halve_at_epoch_1 = 80
halve_at_epoch_2 = 120
halve_at_epoch_3 = 150

gamma = 0.024

TRAIN = False

# Functions
def vae_loss(z_mean, z_log_var):
    def loss(x_true, x_pred):
        x_true = K.reshape(x_true, (-1, np.prod(input_dim)))
        x_pred = K.reshape(x_pred, (-1, np.prod(input_dim)))

        L_rec = 0.5 * K.sum(K.square(x_true - x_pred), axis=-1)
        L_KL = 0.5 * K.sum(K.square(z_mean) + K.exp(z_log_var) - 1 - z_log_var, axis=-1)

        return K.mean(L_rec + gamma / 3 * L_KL)
    return loss

def recon(x_true, x_pred):
    x_true = K.reshape(x_true, (-1, np.prod(input_dim)))
    x_pred = K.reshape(x_pred, (-1, np.prod(input_dim)))

    return K.mean(0.5 * K.sum(K.square(x_true - x_pred), axis=-1))

def KL(z_mean, z_log_var):
    def kl(x_true, x_pred):
        return K.mean(0.5 * K.sum(K.square(z_mean) + K.exp(z_log_var) - 1 - z_log_var, axis=-1))
    return kl

def sampling(args):
    z_mean, z_log_var = args
    eps = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim))

    return z_mean + K.exp(0.5 * z_log_var) * eps

# Dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

x_train = x_train.astype('float32')/255
x_test  = x_test.astype('float32')/255

print('x_train shape: ' + str(x_train.shape))
print('x_test shape: ' + str(x_test.shape))

# Model Architecture
# ENCODER
x = Input(shape=input_dim) # Shape (32, 32, 3)

h = Conv2D(128, 4, strides=(2, 2), padding='same')(x) # Shape (16, 16, 64)
h = BatchNormalization()(h)
h = ReLU()(h)

h = Conv2D(256, 4, strides=(2, 2), padding='same')(h) # Shape (8, 8, 128)
h = BatchNormalization()(h)
h = ReLU()(h)

h = Conv2D(512, 4, strides=(2, 2), padding='same')(h) # Shape (4, 4, 256)
h = BatchNormalization()(h)
h = ReLU()(h)

h = Conv2D(1024, 4, strides=(2, 2), padding='same')(h) # Shape (2, 2, 512)
h = BatchNormalization()(h)
h = ReLU()(h)

h = Flatten()(h)

z_mean = Dense(latent_dim)(h)
z_log_var = Dense(latent_dim)(h)
z = Lambda(sampling)([z_mean, z_log_var])

encoder = Model(x, [z, z_mean, z_log_var])

# DECODER
z_in = Input(shape=(latent_dim, ))

h = Dense(8 * 8 * 1024)(z_in)
h = Reshape((8, 8, 1024))(h)
h = BatchNormalization()(h)
h = ReLU()(h)

h = Conv2DTranspose(512, 4, strides=(2, 2), padding='same')(h) # Shape (4, 4, 256)
h = BatchNormalization()(h)
h = ReLU()(h)

h = Conv2DTranspose(256, 4, strides=(2, 2), padding='same')(h) # Shape (8, 8, 128)
h = BatchNormalization()(h)
h = ReLU()(h)

x_decoded = Conv2DTranspose(3, 4, strides=(1, 1), padding='same', activation='sigmoid')(h) # Shape (32, 32, 3)

decoder = Model(z_in, x_decoded)

# VAE
x_recon = decoder(z)

vae = Model(x, x_recon)

# Compile model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay
steps_per_epoch = 45000 / batch_size
lr_schedule = PiecewiseConstantDecay([steps_per_epoch * halve_at_epoch_1, steps_per_epoch * halve_at_epoch_2], 
                                     [initial_lr, initial_lr/2, initial_lr/4])
optimizer = Adam(learning_rate=lr_schedule)

vae.compile(optimizer=optimizer, loss=vae_loss(z_mean, z_log_var), metrics=['mse', KL(z_mean, z_log_var)])

if TRAIN:
    # Fit model
    vae.compile(optimizer=Adam(learning_rate=0.00001), loss=vae_loss(z_mean, z_log_var), metrics=['mse', KL(z_mean, z_log_var)])
    hist = vae.fit(x_train, x_train, batch_size=batch_size, epochs=5, verbose=1, validation_split=0.1)

else:
    vae.load_weights('drive/MyDrive/Weights/naive_VAE_CIFAR10_fixedgamma.h5')

import time
batch = 100
n_samples = 10000
nb = n_samples // batch
n_epochs = 50

times = []
for epoch in range(n_epochs):
    print('Processing ', epoch + 1, '-th epoch.')
    initial_time = time.time()
    for i in range(nb):
        vae.predict(x_test[i * batch : (i+1) * batch])
    times.append(time.time() - initial_time)
times = np.array(times)

print('Forward time: ', np.mean(times), ' +- ', np.std(times))

"""# Save training history"""

if TRAIN:
    loss = hist.history['loss']
    recon = hist.history['recon']
    kl = hist.history['kl']

    val_loss = hist.history['val_loss']
    val_recon = hist.history['val_recon']
    val_kl = hist.history['val_kl']

    from numpy import savetxt
    # save history
    data = np.asarray([loss, recon, kl, val_loss, val_recon, val_kl])
    savetxt('naive_VAE_CIFAR10_fixedgamma.csv', data, delimiter=',')

    vae.save_weights('naive_VAE_CIFAR10_fixedgamma.h5')
else:
    vae.load_weights('naive_VAE_CIFAR10_fixedgamma.h5')

print(encoder.summary())
print(decoder.summary())

"""# Generation and Reconstruction"""

# Reconstruction
n = 10
digit_size = input_dim[0]

x_recon = vae.predict(x_test, batch_size=batch_size)
figure = np.zeros((2 * digit_size, n * digit_size, 3))

for i in range(n):
    figure[:digit_size, i * digit_size: (i+1) * digit_size, :] = x_test[i]
    figure[digit_size:, i * digit_size: (i+1) * digit_size, :] = x_recon[i]

plt.style.use('default')
plt.imshow(figure)
plt.savefig('naive_VAE_CIFAR10_fixedgamma_reconstruction.png')
plt.show()

# Reconstruction
n = 10
digit_size = input_dim[0]

x_recon = vae.predict(x_train[:n])
figure = np.zeros((2 * digit_size, n * digit_size, 3))

for i in range(n):
    figure[:digit_size, i * digit_size: (i+1) * digit_size, :] = x_train[i]
    figure[digit_size:, i * digit_size: (i+1) * digit_size, :] = x_recon[i]

plt.style.use('default')
plt.imshow(figure)
plt.savefig('naive_VAE_CIFAR10_fixedgamma_reconstruction.png')
plt.show()

mse_train = np.mean(np.square(x_train - vae.predict(x_train)))
mse_test = np.mean(np.square(x_test - vae.predict(x_test)))

print('mse train: ', mse_train)
print('mse test: ', mse_test)

# Generation
n = 10 #figure with n x n digits
digit_size = 32
figure = np.zeros((digit_size * n, digit_size * n, 3))
# we will sample n points randomly sampled

z_sample = np.random.normal(size=(n**2, latent_dim), scale=1)
for i in range(n):
    for j in range(n):
        x_decoded = decoder.predict(np.array([z_sample[i + n * j]]))
        figure[i * digit_size: (i + 1) * digit_size,
            j * digit_size: (j + 1) * digit_size, :] = x_decoded

plt.figure(figsize=(10, 10))
plt.imshow(figure)
plt.savefig('naive_VAE_CIFAR10_fixedgamma_generation.png')
plt.show()

T = 10
N = 15

s = 10
k = 100

X_final = [0] * N
for n in range(N):
    a = n + s
    b = n + k

    x_a = x_train[a] #x_test[a]
    x_b = x_train[b] #x_test[b]

    z_a = encoder.predict(np.reshape(x_a, (1, digit_size, digit_size, 3)))[0]
    z_b = encoder.predict(np.reshape(x_b, (1, digit_size, digit_size, 3)))[0]


    X = [0] * T
    for i in range(T):
        t = i/T
        z = t * z_a + (1 - t) * z_b

        X[i] = decoder.predict(z)
    
    X_final[n] = X

digit_size = 32
figure = np.zeros((N * digit_size, T * digit_size, 3))
for n in range(N):
    for i in range(T):
        figure[n * digit_size : (n+1) * digit_size, i * digit_size : (i+1) * digit_size, :] = X_final[n][i][0, :, :, :]

plt.figure(figsize=(10, 10))
plt.imshow(figure)
plt.savefig('naive_VAE_CIFAR10_fixedgamma_interpolation.png')
plt.show()

# We want to investigate overfitting
def get_distance(x, y_vec):
    digit_size = x.shape[0]
    x = np.reshape(x, (digit_size**2*3, ))
    y_vec = np.reshape(y_vec, (-1, digit_size**2*3, ))

    res = np.zeros((y_vec.shape[0], ))
    for j in range(y_vec.shape[0]):
        res[j] = np.mean(np.square(x - y_vec[j]))

    return res

N = 3 # Images we need to check
digit_size = 32
z = np.random.normal(size=(N, latent_dim))
x_gen = decoder.predict(z)
figure = np.zeros((N * digit_size, 2 * digit_size, 3))
for i in range(N):
    x = x_gen[i]
    distances = get_distance(x, x_train)
    m = np.min(distances)
    i_m = np.argmin(distances)

    figure[i * digit_size : (i+1) * digit_size, : digit_size, :] = x
    figure[i * digit_size : (i+1) * digit_size, digit_size :] = x_train[i_m]

plt.figure(figsize=(10, 10))
plt.imshow(figure)
plt.savefig('naive_VAE_CIFAR10_fixedgamma_NN.png')
plt.show()

# Hand_crafted Generation
T = 10
L = 5
s = 1
#dir = np.random.normal(0, sigma_p, (1, latent_dim))
dir = encoder.predict(x_train[s:s+1])[0]
dir = dir[0, :]

print(np.linalg.norm(dir))

interval = np.linspace(1, L, T)
z_vec = []
for i in interval:
    z_vec.append(i * dir)
z_vec = np.array(z_vec)
x_pred = decoder.predict(z_vec)

figure = np.zeros((digit_size * 2, digit_size * T, 3))
for t in range(T):
    figure[: digit_size, t * digit_size : (t+1) * digit_size, :] = x_pred[t, :, :, :]


################
move = np.zeros((latent_dim, ))
move[0] = 1
move[1] = 1
move[2] = -1

L = 25

interval = np.linspace(0, L, T)
z_vec = []
for i in interval:
    z_vec.append(i * move + dir)
z_vec = np.array(z_vec)
x_pred = decoder.predict(z_vec)

for t in range(T):
    figure[digit_size:, t * digit_size : (t+1) * digit_size, :] = x_pred[t, :, :, :]

fig = plt.imshow(figure)
fig.axes.get_xaxis().set_visible(False)
fig.axes.get_yaxis().set_visible(False)
plt.ylabel('Trans \t Ray')
plt.savefig('CIFAR10_move_generation.png')
plt.show()

"""# Metrics Evaluation

First of all, we want to evaluate the ability of the model of generate high quality samples.
"""

import numpy as np
from scipy.linalg import sqrtm
from keras.applications.inception_v3 import InceptionV3
import tensorflow as tf
#from keras.applications.inception_v3 import preprocess_input
#from skimage.transform import resize
#from tensorflow.keras.models import load_model
#import os
#from matplotlib import pyplot

# prepare the inception v3 model
model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3), weights='imagenet')

def get_inception_activations(inps, batch_size=100):
    n_batches = inps.shape[0]//batch_size
    act = np.zeros([inps.shape[0], 2048], dtype = np.float32)
    for i in range(n_batches):
        inp = inps[i * batch_size:(i + 1) * batch_size]
        inpr = tf.image.resize(inp, (299, 299))
        act[i * batch_size:(i + 1) * batch_size] = model.predict(inpr,steps=1)
        
        print('Processed ' + str((i+1) * batch_size) + ' images.')
    return act

def get_fid(images1, images2, use_preprocessed_test=False):
    print(images1.shape)
    print(images2.shape)
    print(type(images1))
    # calculate activations
    if not use_preprocessed_test:
        act1 = get_inception_activations(images1,batch_size=100)
    else:
        import pickle
        with open('./drive/MyDrive/Weights/test_FID.pickle', 'rb') as test_fid:
            act1 = pickle.load(test_fid)
    #print(np.shape(act1))
    act2 = get_inception_activations(images2,batch_size=100)
    # compute mean and covariance statistics
    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
    # calculate sum squared difference between means
    ssdiff = np.sum((mu1 - mu2)**2.0)
    # compute sqrt of product between cov
    covmean = sqrtm(sigma1.dot(sigma2))
    # check and correct imaginary numbers from sqrt
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    # calculate score
    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
    return fid

z_sample = np.random.normal(0, 1, size=(x_test.shape[0], latent_dim))
x_gen = decoder.predict(z_sample)

fid = get_fid(2 * x_test - 1, 2 * vae.predict(x_train[:10000]) - 1, use_preprocessed_test=True)
print('\n FID: %.3f' % fid)

act = get_inception_activations(2 * x_test - 1)

import pickle
with open('test_FID.pickle', 'wb') as test_fid:
    pickle.dump(act, test_fid)

"""### Deactivated Latent Variables, Variance Loss and Variance Law

"""

def count_deactivated_variables(z_var, treshold = 0.8):
    z_var = np.mean(z_var, axis=0)

    return np.sum(z_var > treshold)

def loss_variance(x_true, x_recon):
    x_true = np.reshape(x_true, (-1, np.prod(x_true.shape[1:])))
    x_recon = np.reshape(x_recon, (-1, np.prod(x_recon.shape[1:])))

    var_true = np.mean(np.var(x_true, axis=1), axis=0)
    var_recon = np.mean(np.var(x_recon, axis=1), axis=0)

    return np.abs(var_true - var_recon)

########################################################################################################################
# SHOW THE RESULTS
########################################################################################################################

_, z_mean, z_log_var = encoder.predict(x_test, batch_size=batch_size)
z_var = np.exp(z_log_var)
n_deact = count_deactivated_variables(z_var)
print('We have a total of ', latent_dim, ' latent variables. ', count_deactivated_variables(z_var), ' of them are deactivated')

var_law = np.mean(np.var(z_mean, axis=0) + np.mean(z_var, axis=0))
print('Variance law has a value of: ', var_law)

x_recon = vae.predict(x_train, batch_size=batch_size)
print('We lost ', loss_variance(x_test, x_recon), 'Variance of the original data')

# We want to verify if q(z) = p(z).

# Moments Matching
# Generate samples from q(z) and for p(z)
# p(z) = N(0, I)
# q(z) = E_q(x)[q(z|x)]
#
# For every moment we compare the log-moments
n = len(x_test)

p_samples = np.random.normal(size=(n, latent_dim))
q_samples = encoder.predict(x_test, batch_size=batch_size)[0]


from scipy.stats import moment
# First moment matching:
p_first_moment = np.mean(np.mean(p_samples, axis=0))
q_first_moment = np.mean(np.mean(q_samples, axis=0))

print("\n")
print("First log-moment of p(z): " + str(p_first_moment))
print("First log-moment of q(z): " + str(q_first_moment))
print("\n")

# Second moment matching:
p_second_moment = np.mean(np.var(p_samples, axis=0))
q_second_moment = np.mean(np.var(q_samples, axis=0))

print("\n")
print("Second log-moment of p(z): " + str(p_second_moment))
print("Second log-moment of q(z): " + str(q_second_moment))
print("\n")

# Thid moment matching:
p_third_moment = np.mean(moment(p_samples, moment=3, axis=0))
q_third_moment = np.mean(moment(q_samples, moment=3, axis=0))

print("\n")
print("Third log-moment of p(z): " + str(p_third_moment))
print("Third log-moment of q(z): " + str(q_third_moment))
print("\n")

x_recon = vae.predict(x_train)

print(np.mean(np.square(x_recon - x_train)))

# Second Stage VAE
from keras.layers import Concatenate
SECOND_TRAIN = False
# Loss
def second_stage_loss(u_mean, u_log_var):
    def loss(x_true, x_pred):
        import tensorflow as tf
        normalize_z = tf.nn.l2_normalize(x_true, 1)
        normalize_z_hat = tf.nn.l2_normalize(x_pred, 1)
        cos_similarity = - K.sum(normalize_z * normalize_z_hat, axis=-1)
        L_KL = 0.5 * K.sum(K.square(u_mean) + K.exp(u_log_var) - 1 - u_log_var, axis=-1)

        return K.mean(cos_similarity + .007 * L_KL)
    return loss

def second_recon(x_true, x_pred):
    return K.mean(0.5 * K.sum(K.square(x_true - x_pred), axis=-1))

def second_KL(z_mean, z_log_var):
    def kl(x_true, x_pred):
        return K.mean(0.5 * K.sum(K.square(z_mean) + K.exp(z_log_var) - 1 - z_log_var, axis=-1))
    return kl

def cos_sim(x_true, x_pred):
    import tensorflow as tf
    normalize_z = tf.nn.l2_normalize(x_true, 1)
    normalize_z_hat = tf.nn.l2_normalize(x_pred, 1)
    return K.mean(K.sum(normalize_z * normalize_z_hat, axis=-1))

def second_stage_sampling(args):
    u_mean, u_log_var = args
    eps = K.random_normal(shape=(batch_size, latent_dim))

    return u_mean + eps * K.exp(0.5 * u_log_var)

# Encoder
intermediate_dim = 3000

z = Input(shape=(latent_dim, ))

h = Dense(intermediate_dim, activation='relu')(z)
h = Dense(intermediate_dim, activation='relu')(h)

h = Concatenate()([h, z])

u_mean = Dense(latent_dim)(h)
u_log_var = Dense(latent_dim)(h)

u = Lambda(second_stage_sampling)([u_mean, u_log_var])
second_encoder = Model(z, [u, u_mean, u_log_var])

# Decoder
u_in = Input(shape=(latent_dim, ))

h = Dense(intermediate_dim, activation='relu')(u_in)
h = Dense(intermediate_dim, activation='relu')(h)

h = Concatenate()([h, u_in])

z_decoded = Dense(latent_dim)(h)
second_decoder = Model(u_in, z_decoded)

# VAE
z_reconstructed = second_decoder(u)
second_vae = Model(z, z_reconstructed)

# Compile model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay
steps_per_epoch = 45000 / batch_size
initial_lr = 1e-3
lr_schedule = PiecewiseConstantDecay([steps_per_epoch * halve_at_epoch_1, steps_per_epoch * halve_at_epoch_2, steps_per_epoch * halve_at_epoch_3], 
                                     [initial_lr, initial_lr/10, initial_lr/100, initial_lr/500])
optimizer = Adam(learning_rate=lr_schedule)
second_vae.compile(optimizer=Adam(learning_rate=1e-4), loss=second_stage_loss(u_mean, u_log_var), metrics=[cos_sim, second_KL(u_mean, u_log_var)])

# Generate second stage Dataset
z_train = encoder.predict(x_train)[0]
z_test = encoder.predict(x_test)[0]

if SECOND_TRAIN:
    # Fit second stage VAE
    second_hist = second_vae.fit(z_train, z_train, batch_size=batch_size, epochs=300, verbose=1)
    second_vae.save_weights('second_twostage_VAE_CIFAR10_fixedgamma.h5')
else:
    second_vae.load_weights('drive/MyDrive/Weights/second_twostage_VAE_CIFAR10_fixedgamma.h5')

second_hist = second_vae.fit(z_train, z_train, batch_size=batch_size, epochs=300, verbose=1)
second_vae.save_weights('second_twostage_VAE_CIFAR10_fixedgamma.h5')

# Generation
n = 10 #figure with n x n digits
digit_size = 32
figure = np.zeros((digit_size * n, digit_size * n, 3))
# we will sample n points randomly sampled

"""
We want to sample z from q(z) = E_p(u)[q(z|u)]
p(u) = N(0, I)
"""

u_sample = np.random.normal(size=(n**2, latent_dim), scale=1)
z_sample = second_decoder.predict(u_sample)
std2 = np.std(z_sample, axis=1)
z_sample = z_sample * 0.8 / np.mean(std2)
for i in range(n):
    for j in range(n):
        x_decoded = decoder.predict(np.array([z_sample[i + n * j]]))
        figure[i * digit_size: (i + 1) * digit_size,
            j * digit_size: (j + 1) * digit_size, :] = x_decoded

plt.style.use('default')
plt.figure(figsize=(10, 10))
plt.imshow(figure)
plt.savefig('twostage_VAE_CIFAR10_fixedgamma_generation.png')
plt.show()

import numpy as np
from scipy.linalg import sqrtm
from keras.applications.inception_v3 import InceptionV3
import tensorflow as tf
#from keras.applications.inception_v3 import preprocess_input
#from skimage.transform import resize
#from tensorflow.keras.models import load_model
#import os
#from matplotlib import pyplot

# prepare the inception v3 model
model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3), weights='imagenet')

def get_inception_activations(inps, batch_size=100):
    n_batches = inps.shape[0]//batch_size
    act = np.zeros([inps.shape[0], 2048], dtype = np.float32)
    for i in range(n_batches):
        inp = inps[i * batch_size:(i + 1) * batch_size]
        inpr = tf.image.resize(inp, (299, 299))
        act[i * batch_size:(i + 1) * batch_size] = model.predict(inpr,steps=1)
        
        print('Processed ' + str((i+1) * batch_size) + ' images.')
    return act

def get_fid(images1, images2, use_preprocessed_test=False):
    print(images1.shape)
    print(images2.shape)
    print(type(images1))
    # calculate activations
    if not use_preprocessed_test:
        act1 = get_inception_activations(images1,batch_size=100)
    else:
        import pickle
        with open('./drive/MyDrive/Weights/test_FID.pickle', 'rb') as test_fid:
            act1 = pickle.load(test_fid)
    #print(np.shape(act1))
    act2 = get_inception_activations(images2,batch_size=100)
    # compute mean and covariance statistics
    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
    # calculate sum squared difference between means
    ssdiff = np.sum((mu1 - mu2)**2.0)
    # compute sqrt of product between cov
    covmean = sqrtm(sigma1.dot(sigma2))
    # check and correct imaginary numbers from sqrt
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    # calculate score
    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
    return fid

u_sample = np.random.normal(0, 1, size=(x_test.shape[0], latent_dim))
z_sample = second_decoder.predict(u_sample)
z_sample = second_decoder.predict(u_sample)
std2 = np.std(z_sample, axis=1)
z_sample = z_sample * 0.8 / np.mean(std2)
x_gen = decoder.predict(z_sample)

fid = get_fid(2 * x_test - 1, 2 * x_gen - 1, use_preprocessed_test=True)
print('\n FID: %.3f' % fid)

# Learn latent space distribution
z_train = encoder.predict(x_train)[0]

prior_for_qz = "GMM" # Choose between GMM or Gaussian
if prior_for_qz == "GMM":
    from sklearn.mixture import GaussianMixture

    z_density = GaussianMixture(n_components=10, max_iter=100)
    z_density.fit(z_train)

    print("Learned GMM")
elif prior_for_qz == "Gaussian":
    from scipy.stats import norm

    mean, std = norm.fit(z_train) # z_train is fitted to a gaussian N(mean, std)
    print("Learned Gaussian")
else:
    print("Distribution not found")

# Generation
n = 10 #figure with n x n digits
digit_size = 32
figure = np.zeros((digit_size * n, digit_size * n, 3))
# we will sample n points randomly sampled

if prior_for_qz == "GMM":
    z_sample = z_density.sample(n**2)
    z_sample = z_sample[0]
elif prior_for_qz == "Gaussian":
    z_sample = np.random.normal(size=(n**2, latent_dim), loc=mean, scale=std)
else:
    print("Distribution not found")

for i in range(n):
    for j in range(n):
        x_decoded = decoder.predict(np.array([z_sample[i + n * j]]))
        figure[i * digit_size: (i + 1) * digit_size,
            j * digit_size: (j + 1) * digit_size] = x_decoded

plt.figure(figsize=(10, 10))
plt.imshow(figure)
plt.savefig('L2-RAE_CIFAR10_generation.png')
plt.show()

import numpy as np
from scipy.linalg import sqrtm
from keras.applications.inception_v3 import InceptionV3
import tensorflow as tf
#from keras.applications.inception_v3 import preprocess_input
#from skimage.transform import resize
#from tensorflow.keras.models import load_model
#import os
#from matplotlib import pyplot

# prepare the inception v3 model
model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3), weights='imagenet')

def get_inception_activations(inps, batch_size=100):
    n_batches = inps.shape[0]//batch_size
    act = np.zeros([inps.shape[0], 2048], dtype = np.float32)
    for i in range(n_batches):
        inp = inps[i * batch_size:(i + 1) * batch_size]
        inpr = tf.image.resize(inp, (299, 299))
        act[i * batch_size:(i + 1) * batch_size] = model.predict(inpr,steps=1)
        
        print('Processed ' + str((i+1) * batch_size) + ' images.')
    return act

def get_fid(images1, images2, use_preprocessed_test=False):
    print(images1.shape)
    print(images2.shape)
    print(type(images1))
    # calculate activations
    if not use_preprocessed_test:
        act1 = get_inception_activations(images1,batch_size=100)
    else:
        import pickle
        with open('./drive/MyDrive/Weights/test_FID.pickle', 'rb') as test_fid:
            act1 = pickle.load(test_fid)
    #print(np.shape(act1))
    act2 = get_inception_activations(images2,batch_size=100)
    # compute mean and covariance statistics
    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
    # calculate sum squared difference between means
    ssdiff = np.sum((mu1 - mu2)**2.0)
    # compute sqrt of product between cov
    covmean = sqrtm(sigma1.dot(sigma2))
    # check and correct imaginary numbers from sqrt
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    # calculate score
    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
    return fid

if prior_for_qz == "GMM":
    z_sample = z_density.sample(x_test.shape[0])
elif prior_for_qz == "Gaussian":
    z_sample = np.random.normal(size=(x_test.shape[0], latent_dim), loc=mean, scale=std)
else:
    print("Distribution not found")
x_gen = decoder.predict(z_sample)

fid = get_fid(2 * x_test - 1, 2 * x_gen - 1, use_preprocessed_test=True)
print('\n FID: %.3f' % fid)
"""
x_gen = vae.predict(x_train[:10000])
rec_fid = get_fid(2 * x_test - 1, 2 * x_gen - 1, use_preprocessed_test=True)
print('\n Train FID: %.3f' % rec_fid)
"""